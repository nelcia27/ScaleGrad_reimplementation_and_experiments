{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9d1bfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (1.17.0)\n",
      "Requirement already satisfied: transformers in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (4.16.0.dev0)\n",
      "Requirement already satisfied: pandas in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from datasets) (1.3.2)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from datasets) (4.0.1)\n",
      "Requirement already satisfied: multiprocess in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: dill in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: packaging in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from datasets) (21.0)\n",
      "Requirement already satisfied: importlib-metadata in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from datasets) (4.10.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from datasets) (2.25.1)\n",
      "Requirement already satisfied: aiohttp in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from datasets) (2022.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from datasets) (1.21.1)\n",
      "Requirement already satisfied: xxhash in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: sacremoses in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from transformers) (2021.7.6)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from transformers) (0.11.2)\n",
      "Requirement already satisfied: filelock in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (d:\\ewa - studia\\python\\python3.7\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\ewa - studia\\python\\python3.7\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\ewa - studia\\python\\python3.7\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\ewa - studia\\python\\python3.7\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\ewa - studia\\python\\python3.7\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chardet<5,>=3.0.2 in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
      "Requirement already satisfied: colorama in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from aiohttp->datasets) (5.2.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from aiohttp->datasets) (2.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from importlib-metadata->datasets) (3.5.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from pandas->datasets) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: joblib in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in d:\\ewa - studia\\python\\python3.7\\lib\\site-packages (from sacremoses->transformers) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b13b11bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr for BERT 0.002\n",
    "# lr for decoder 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20c13a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from C:\\Users\\ewako\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\wikitext\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126 (last modified on Thu Jan 20 23:18:06 2022) since it couldn't be found locally at wikitext., or remotely on the Hugging Face Hub.\n",
      "Reusing dataset wikitext (C:\\Users\\ewako\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-raw-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07cc840da01a457aaeff1eccdcdac16d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "#datasets = load_dataset(\"text\", data_files={\"train\": 'train.txt', \"validation\": 'valid.txt'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5498d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3ac353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilgpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cfd2538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9daa8d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"d:\\ewa - studia\\python\\python3.7\\lib\\site-packages\\multiprocess\\pool.py\", line 121, in worker\n    result = (True, func(*args, **kwds))\n  File \"d:\\ewa - studia\\python\\python3.7\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 518, in wrapper\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n  File \"d:\\ewa - studia\\python\\python3.7\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 485, in wrapper\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n  File \"d:\\ewa - studia\\python\\python3.7\\lib\\site-packages\\datasets\\fingerprint.py\", line 411, in wrapper\n    out = func(self, *args, **kwargs)\n  File \"d:\\ewa - studia\\python\\python3.7\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 2473, in _map_single\n    offset=offset,\n  File \"d:\\ewa - studia\\python\\python3.7\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 2357, in apply_function_on_filtered_inputs\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n  File \"d:\\ewa - studia\\python\\python3.7\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 2052, in decorated\n    result = f(decorated_item, *args, **kwargs)\n  File \"C:\\Users\\ewako\\AppData\\Local\\Temp/ipykernel_1012/1471758222.py\", line 2, in tokenize_function\nNameError: name 'tokenizer' is not defined\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1012/2919175032.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtokenized_datasets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenize_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_proc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremove_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\ewa - studia\\python\\python3.7\\lib\\site-packages\\datasets\\dataset_dict.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, function, with_indices, input_columns, batched, batch_size, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[0;32m    510\u001b[0m                     \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdesc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m                 )\n\u001b[1;32m--> 512\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    513\u001b[0m             }\n\u001b[0;32m    514\u001b[0m         )\n",
      "\u001b[1;32md:\\ewa - studia\\python\\python3.7\\lib\\site-packages\\datasets\\dataset_dict.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    510\u001b[0m                     \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdesc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m                 )\n\u001b[1;32m--> 512\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    513\u001b[0m             }\n\u001b[0;32m    514\u001b[0m         )\n",
      "\u001b[1;32md:\\ewa - studia\\python\\python3.7\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   2203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2204\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0masync_result\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2205\u001b[1;33m                         \u001b[0mtransformed_shards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masync_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2207\u001b[0m             assert (\n",
      "\u001b[1;32md:\\ewa - studia\\python\\python3.7\\lib\\site-packages\\multiprocess\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    655\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    656\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 657\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    659\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7767d5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "def getNovelMask(target, vocab_size):\n",
    "    b, l = target.size()\n",
    "    zeros = torch.zeros(b, l, vocab_size).to(target.device)\n",
    "    ones = torch.ones(b, l, vocab_size).to(target.device)\n",
    "\n",
    "    target_index = target.unsqueeze(1).expand(b, l, l).transpose(-2, -1).triu().transpose(-2, -1)\n",
    "    matrix = zeros.scatter_add_(2, target_index, ones)\n",
    "    matrix[:, :, 0] = 0\n",
    "    summ_true = torch.tensor(range(1, l + 1)).unsqueeze(0).float().to(target.device)\n",
    "    summ_now = torch.sum(matrix, dim=-1)\n",
    "    diff = summ_true - summ_now\n",
    "    matrix[:, :, 0] = diff\n",
    "    matrix = torch.cat((torch.zeros(b, 1, vocab_size).to(target.device), matrix[:, :-1, :]), 1)\n",
    "    novel_mask = matrix < 1.\n",
    "\n",
    "    return novel_mask\n",
    "\n",
    "\n",
    "def sg_loss(model, batch, gamma, output):\n",
    "    longer_sample = batch['input_ids'][0]\n",
    "    inp = longer_sample\n",
    "    model_output = output\n",
    "    target = batch['labels'][0]\n",
    "    logits = model_output[1]\n",
    "\n",
    "    # ScaleGrad\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    novel_mask = getNovelMask(target.unsqueeze(0), logits.size(-1))\n",
    "    rep_mask = ~novel_mask\n",
    "\n",
    "    new_probs = probs * novel_mask * gamma + probs * rep_mask + 1e-8\n",
    "    new_probs = F.normalize(new_probs, p=1, dim=-1)\n",
    "    lprobs = torch.log(new_probs)\n",
    "    lprobs_flatten = torch.unsqueeze(lprobs[0,:,:],0)\n",
    "    assert lprobs_flatten.size(0) == 1, 'Nonflat sequence error'\n",
    "    loss = F.nll_loss(lprobs_flatten[0], target, reduction='sum')\n",
    "    true_token_logits = -F.nll_loss(logits[0], target, reduction='none')\n",
    "    ntokens = inp.numel()\n",
    "\n",
    "    return loss / ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9652d913",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.2\n",
    "class ScaleGradTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        loss = sg_loss(model, inputs, gamma, outputs)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8758e067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70ef76fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-wikitext2\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    "    num_train_epochs = 1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f87c3479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "# source: https://huggingface.co/docs/transformers/training\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # TODO: zmienić\n",
    "    metric = load_metric(\"accuracy\")\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa2d4bc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1012/157012083.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer = ScaleGradTrainer(\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mcompute_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[1;33m,\u001b[0m    \u001b[1;31m# compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# loss jest podstawowym założeniem (było, że jeśli tutaj None, to only_loss na true)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = ScaleGradTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,    # compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None, \n",
    "    # loss jest podstawowym założeniem (było, że jeśli tutaj None, to only_loss na true)\n",
    "    train_dataset=None,\n",
    "    eval_dataset=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfcac0c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1012/4130767944.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0meval_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b51f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: na inne miary?\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9567d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import list_metrics\n",
    "metrics_list = list_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a636bc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accuracy', 'bertscore', 'bleu', 'bleurt', 'cer', 'chrf', 'code_eval', 'comet', 'competition_math', 'coval', 'cuad', 'f1', 'gleu', 'glue', 'google_bleu', 'indic_glue', 'matthews_correlation', 'mauve', 'meteor', 'pearsonr', 'precision', 'recall', 'rouge', 'sacrebleu', 'sari', 'seqeval', 'spearmanr', 'squad', 'squad_v2', 'super_glue', 'ter', 'wer', 'wiki_split', 'xnli']\n"
     ]
    }
   ],
   "source": [
    "print(metrics_list) # nie widzę Rep/l and uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6a8e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tu są metryki z torcha: https://torchmetrics.readthedocs.io/en/latest/references/modules.html#text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
